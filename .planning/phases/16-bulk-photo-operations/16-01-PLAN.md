---
phase: 16-bulk-photo-operations
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/db/migrations/008_add_perceptual_hash.sql
  - backend/db/queries/item_photos.sql
  - backend/internal/domain/warehouse/itemphoto/entity.go
  - backend/internal/domain/warehouse/itemphoto/repository.go
  - backend/internal/domain/warehouse/itemphoto/service.go
  - backend/internal/domain/warehouse/itemphoto/handler.go
  - backend/internal/domain/warehouse/itemphoto/dto.go
  - backend/internal/infra/imageprocessor/hasher.go
autonomous: true

must_haves:
  truths:
    - "Bulk delete endpoint removes multiple photos in single request"
    - "Bulk caption endpoint updates captions for multiple photos"
    - "Zip download endpoint streams all item photos as downloadable archive"
    - "Duplicate check endpoint returns similar photos before upload"
  artifacts:
    - path: "backend/db/migrations/008_add_perceptual_hash.sql"
      provides: "perceptual_hash column for duplicate detection"
      contains: "perceptual_hash BIGINT"
    - path: "backend/db/queries/item_photos.sql"
      provides: "Bulk operation queries"
      exports: ["GetItemPhotosByIDs", "BulkDeleteItemPhotos", "BulkUpdateCaptions", "GetPhotosWithHashes"]
    - path: "backend/internal/infra/imageprocessor/hasher.go"
      provides: "Perceptual hash generation using goimagehash"
      min_lines: 30
    - path: "backend/internal/domain/warehouse/itemphoto/handler.go"
      provides: "Bulk operation HTTP handlers"
      contains: "HandleBulkDelete"
  key_links:
    - from: "handler.go"
      to: "service.go"
      via: "BulkDeletePhotos, BulkUpdateCaptions, DownloadPhotosZip, CheckDuplicates"
      pattern: "svc\\.(BulkDelete|BulkUpdate|Download|CheckDuplicates)"
    - from: "service.go"
      to: "hasher.go"
      via: "GeneratePerceptualHash for duplicate detection"
      pattern: "hasher\\.Generate"
---

<objective>
Add backend infrastructure for bulk photo operations including bulk delete, bulk caption update, zip download, and duplicate detection.

Purpose: Enable efficient multi-photo management by providing atomic bulk operations and proactive duplicate warnings during upload.
Output: Four new API endpoints and perceptual hashing integration for duplicate detection.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-bulk-photo-operations/16-RESEARCH.md

# Existing item photo infrastructure
@backend/internal/domain/warehouse/itemphoto/entity.go
@backend/internal/domain/warehouse/itemphoto/service.go
@backend/internal/domain/warehouse/itemphoto/handler.go
@backend/internal/domain/warehouse/itemphoto/repository.go
@backend/db/queries/item_photos.sql
@backend/db/migrations/007_add_thumbnail_processing.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database schema and sqlc queries for bulk operations</name>
  <files>
    - backend/db/migrations/008_add_perceptual_hash.sql
    - backend/db/queries/item_photos.sql
  </files>
  <action>
Create migration 008_add_perceptual_hash.sql:
- Add `perceptual_hash BIGINT` column to warehouse.item_photos (nullable, existing photos won't have it)
- Add index: `CREATE INDEX idx_item_photos_hash ON warehouse.item_photos(perceptual_hash) WHERE perceptual_hash IS NOT NULL`
- Add column comment explaining dHash purpose

Add sqlc queries to item_photos.sql:
1. `GetItemPhotosByIDs :many` - Get multiple photos by ID array with workspace verification
   ```sql
   SELECT * FROM warehouse.item_photos
   WHERE id = ANY($1::uuid[]) AND workspace_id = $2
   ORDER BY display_order ASC;
   ```

2. `BulkDeleteItemPhotos :exec` - Delete multiple photos by ID array
   ```sql
   DELETE FROM warehouse.item_photos
   WHERE id = ANY($1::uuid[]) AND workspace_id = $2;
   ```

3. `BulkUpdateCaptions :exec` - Update caption for single photo (called in loop within transaction)
   ```sql
   UPDATE warehouse.item_photos
   SET caption = $2, updated_at = now()
   WHERE id = $1 AND workspace_id = $3;
   ```

4. `GetPhotosWithHashes :many` - Get photos with perceptual hashes for duplicate detection
   ```sql
   SELECT id, perceptual_hash, thumbnail_medium_path FROM warehouse.item_photos
   WHERE item_id = $1 AND workspace_id = $2 AND perceptual_hash IS NOT NULL;
   ```

5. `UpdatePerceptualHash :exec` - Set hash after upload processing
   ```sql
   UPDATE warehouse.item_photos
   SET perceptual_hash = $2, updated_at = now()
   WHERE id = $1;
   ```

Run: `mise run sqlc` to regenerate Go code.
Run: `mise run migrate` to apply migration.
  </action>
  <verify>
    - `mise run sqlc` completes without errors
    - `mise run migrate` applies migration 008
    - Generated sqlc code includes new query functions
  </verify>
  <done>
    - perceptual_hash column exists in item_photos table
    - All 5 new sqlc queries are generated and available
  </done>
</task>

<task type="auto">
  <name>Task 2: Perceptual hash generator and service bulk methods</name>
  <files>
    - backend/internal/infra/imageprocessor/hasher.go
    - backend/internal/domain/warehouse/itemphoto/entity.go
    - backend/internal/domain/warehouse/itemphoto/repository.go
    - backend/internal/domain/warehouse/itemphoto/service.go
  </files>
  <action>
**Create hasher.go** in imageprocessor package:
```go
package imageprocessor

import (
    "context"
    "fmt"
    "github.com/corona10/goimagehash"
    "github.com/disintegration/imaging"
)

// Hasher generates perceptual hashes for images
type Hasher struct{}

// NewHasher creates a new perceptual hasher
func NewHasher() *Hasher {
    return &Hasher{}
}

// GenerateHash generates a dHash (difference hash) for an image file
// Returns the 64-bit hash value
func (h *Hasher) GenerateHash(ctx context.Context, imagePath string) (uint64, error) {
    img, err := imaging.Open(imagePath)
    if err != nil {
        return 0, fmt.Errorf("failed to open image: %w", err)
    }

    hash, err := goimagehash.DifferenceHash(img)
    if err != nil {
        return 0, fmt.Errorf("failed to generate hash: %w", err)
    }

    return hash.GetHash(), nil
}

// CompareHashes returns the Hamming distance between two hashes
// Lower distance = more similar (0 = identical)
func (h *Hasher) CompareHashes(hash1, hash2 uint64) int {
    h1, _ := goimagehash.NewImageHash(hash1, goimagehash.DHash)
    h2, _ := goimagehash.NewImageHash(hash2, goimagehash.DHash)
    distance, _ := h1.Distance(h2)
    return distance
}

// SimilarityThresholds
const (
    ThresholdIdentical = 0  // Exact match
    ThresholdDefinite  = 5  // Definitely a duplicate
    ThresholdSimilar   = 10 // Similar image (warning worthy)
)
```

**Update entity.go** - Add PerceptualHash field:
```go
type ItemPhoto struct {
    // ... existing fields
    PerceptualHash *int64 // dHash for duplicate detection
}
```

**Update repository.go** - Add methods:
- `GetByIDs(ctx, ids []uuid.UUID, workspaceID uuid.UUID) ([]*ItemPhoto, error)`
- `BulkDelete(ctx, ids []uuid.UUID, workspaceID uuid.UUID) error`
- `BulkUpdateCaptions(ctx, updates []CaptionUpdate, workspaceID uuid.UUID) error` where `CaptionUpdate{ID uuid.UUID, Caption *string}`
- `GetPhotosWithHashes(ctx, itemID, workspaceID uuid.UUID) ([]PhotoHash, error)` where `PhotoHash{ID uuid.UUID, Hash *int64, ThumbnailURL *string}`
- `UpdatePerceptualHash(ctx, photoID uuid.UUID, hash int64) error`

**Update service.go** - Add interface methods and implementations:
1. `BulkDeletePhotos(ctx, photoIDs []uuid.UUID, workspaceID uuid.UUID) error`
   - Get photos by IDs first (for storage paths)
   - Delete from database in transaction
   - Queue storage cleanup for each photo's files (original + thumbnails)
   - If any deleted photo was primary, set new primary from remaining

2. `BulkUpdateCaptions(ctx, updates []CaptionUpdate, workspaceID uuid.UUID) error`
   - Validate all photo IDs belong to workspace
   - Update in transaction

3. `GetPhotosForDownload(ctx, itemID, workspaceID uuid.UUID, photoIDs []uuid.UUID) ([]*ItemPhoto, error)`
   - If photoIDs empty, get all photos for item
   - Otherwise filter to specified IDs

4. `CheckDuplicates(ctx, hash uint64, itemID, workspaceID uuid.UUID) ([]DuplicateInfo, error)`
   - Get photos with hashes for item
   - Compare each hash using Hasher.CompareHashes
   - Return duplicates where distance < ThresholdSimilar (10)

Also add `SetHasher(h *Hasher)` setter to Service for dependency injection.
  </action>
  <verify>
    - `go build ./...` compiles successfully
    - `mise run test-unit` passes (no new tests yet, just checking no breaks)
  </verify>
  <done>
    - Hasher generates perceptual hashes from image files
    - Service has BulkDeletePhotos, BulkUpdateCaptions, GetPhotosForDownload, CheckDuplicates methods
    - Repository has corresponding data access methods
  </done>
</task>

<task type="auto">
  <name>Task 3: HTTP handlers for bulk operations and zip download</name>
  <files>
    - backend/internal/domain/warehouse/itemphoto/handler.go
    - backend/internal/domain/warehouse/itemphoto/dto.go
    - backend/go.mod
  </files>
  <action>
**Install goimagehash dependency:**
```bash
cd backend && go get github.com/corona10/goimagehash@v1.1.0
```

**Create dto.go** for request/response types:
```go
package itemphoto

import "github.com/google/uuid"

// BulkDeleteRequest for POST /items/{item_id}/photos/bulk-delete
type BulkDeleteRequest struct {
    PhotoIDs []uuid.UUID `json:"photo_ids" minItems:"1" doc:"Photo IDs to delete"`
}

// BulkCaptionUpdate for batch caption updates
type BulkCaptionUpdate struct {
    PhotoID uuid.UUID `json:"photo_id"`
    Caption *string   `json:"caption"`
}

// BulkCaptionRequest for POST /items/{item_id}/photos/bulk-caption
type BulkCaptionRequest struct {
    Updates []BulkCaptionUpdate `json:"updates" minItems:"1" doc:"Caption updates"`
}

// DuplicateInfo for duplicate detection response
type DuplicateInfo struct {
    PhotoID           uuid.UUID `json:"photo_id"`
    Distance          int       `json:"distance" doc:"Hamming distance (0=identical, <5=definite, <10=similar)"`
    SimilarityPercent int       `json:"similarity_percent" doc:"Similarity as percentage (100=identical)"`
    ThumbnailURL      *string   `json:"thumbnail_url,omitempty"`
}

// DuplicateCheckResponse for POST /items/{item_id}/photos/check-duplicate
type DuplicateCheckResponse struct {
    Duplicates []DuplicateInfo `json:"duplicates"`
    HasExact   bool            `json:"has_exact" doc:"True if exact duplicate found"`
}
```

**Update handler.go** - Add new Huma endpoints in RegisterRoutes:

1. `POST /items/{item_id}/photos/bulk-delete`:
   - Parse BulkDeleteRequest
   - Call svc.BulkDeletePhotos
   - Publish SSE event for each deleted photo
   - Return 204 No Content

2. `POST /items/{item_id}/photos/bulk-caption`:
   - Parse BulkCaptionRequest
   - Call svc.BulkUpdateCaptions
   - Publish SSE event
   - Return 204 No Content

**Add new Chi handler registration** for zip download (streaming response):

3. `GET /items/{item_id}/photos/download`:
   - Optional query param `ids` (comma-separated photo IDs)
   - Set headers: Content-Type: application/zip, Content-Disposition: attachment
   - Create zip.Writer on ResponseWriter
   - For each photo: get from storage, create zip entry, stream content
   - Close zip writer

**Add new Chi handler** for duplicate check (multipart form):

4. `POST /items/{item_id}/photos/check-duplicate`:
   - Parse multipart form with "photo" file
   - Save to temp file
   - Generate hash with Hasher
   - Call svc.CheckDuplicates
   - Return DuplicateCheckResponse

Update RegisterUploadHandler to also register new handlers, or create new RegisterBulkHandlers function.
  </action>
  <verify>
    - `go build ./...` compiles with new handlers
    - `mise run dev` starts server without errors
    - Manual test: `curl -X POST /items/{id}/photos/bulk-delete` with empty array returns 400
  </verify>
  <done>
    - POST /items/{item_id}/photos/bulk-delete endpoint works
    - POST /items/{item_id}/photos/bulk-caption endpoint works
    - GET /items/{item_id}/photos/download streams zip file
    - POST /items/{item_id}/photos/check-duplicate returns duplicate info
  </done>
</task>

</tasks>

<verification>
All backend bulk operations work:
1. Bulk delete: `curl -X POST /items/{id}/photos/bulk-delete -d '{"photo_ids":["uuid1","uuid2"]}'` returns 204
2. Bulk caption: `curl -X POST /items/{id}/photos/bulk-caption -d '{"updates":[{"photo_id":"uuid","caption":"new"}]}'` returns 204
3. Zip download: `curl /items/{id}/photos/download --output photos.zip` produces valid zip
4. Duplicate check: Upload form with photo returns similarity info
</verification>

<success_criteria>
- perceptual_hash column exists in database with index
- All sqlc queries generated and working
- goimagehash dependency installed (go.mod updated)
- Four new API endpoints respond correctly
- Existing photo operations still work (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/16-bulk-photo-operations/16-01-SUMMARY.md`
</output>
